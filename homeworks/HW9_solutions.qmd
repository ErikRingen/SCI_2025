---
title: "SCI 2025: Homework 9"
format: 
  pdf:
    geometry: margin=1in
    fontsize: 12pt
    linestretch: 1.5
    include-in-header:
      text: |
        \usepackage{amsmath}
        \usepackage{amsfonts}
execute:
  echo: true
  warning: false
  message: false
---

## Setup

This homework is based on practice problem 13H1 from *Statistical Rethinking*.

In 1980, a typical Bengali woman could have 5 or more children in her lifetime. By the year 2000, a typical Bengali woman had only 2 or 3. You're going to look at a historical set of data, when contraception was widely available but many families chose not to use it. These data reside in `data(bangladesh)` and come from the 1988 Bangladesh Fertility Survey. Each row is one of 1934 women. There are six variables, but you can focus on two of them for this practice problem:

(1) `district`: ID number of administrative district each woman resided in
(2) `use.contraception`: An indicator (0/1) of whether the woman was using contraception

The first thing to do is ensure that the cluster variable, `district`, is a contiguous set of integers. Recall that these values will be index values inside the model. If there are gaps, you'll have parameters for which there is no data to inform them. Worse, the model probably won't run. Look at the unique values of the `district` variable:

```{r}
library(rethinking)
data(bangladesh)

d <- bangladesh
sort(unique(d$district))
```

District 54 is absent. So district isn't yet a good index variable, because it's not contiguous. This is easy to fix. Just make a new variable that is contiguous. This is enough to do it:

```{r}
d$district_id <- as.integer(as.factor(d$district))
sort(unique(d$district_id))
```

Now there are 60 values, contiguous integers 1 to 60. 

## Question 1:

Now, focus on predicting `use.contraception`, clustered by `district_id`. Fit both:

(1) a traditional fixed-effects model that uses an index variable for district and (2) a multilevel model with varying intercepts for district.

```{r}
model_data <- data.frame(
  district_id = d$district_id,
  use_contraception = d$use.contraception,
  urban = d$urban
)

mean(d$use.contraception) # 0.39
sd(d$use.contraception) # 0.49

m_fixed <- ulam(
  alist(
    use_contraception ~ dbinom(1, p),
    logit(p) <- a[district_id],
    a[district_id] ~ dnorm(0, 1)
  ),
  data = model_data,
  chains = 4,
  cores = 4,
  log_lik = TRUE
)

m_varying <- ulam(
  alist(
    use_contraception ~ dbinom(1, p),
    logit(p) <- a[district_id],
    a_bar ~ dnorm(0, 1),
    a[district_id] ~ dnorm(a_bar, sigma_a),
    sigma_a ~ dexp(1)
  ),
  data = model_data,
  chains = 4,
  cores = 4,
  log_lik = TRUE
)
```

Plot the predicted proportions of women in each district using contraception, for both the fixed-effects model and the varying-effects model. That is, make a plot in which district ID is on the horizontal axis and expected proportion using contraception is on the vertical. Make one plot for each model, or layer them on the same plot, as you prefer. How do the models disagree? Can you explain the pattern of disagreement?

```{r}
pred_fixed <- link(m_fixed, data = data.frame(district_id = sort(unique(model_data$district_id))))
pred_varying <- link(m_varying, data = data.frame(district_id = sort(unique(model_data$district_id))))

fixed_mean <- apply(pred_fixed, 2, mean)
varying_mean <- apply(pred_varying, 2, mean)
fixed_ci <- t(apply(pred_fixed, 2, PI))
varying_ci <- t(apply(pred_varying, 2, PI))

n_districts <- length(unique(model_data$district_id))

plot(
  NULL,
  xlim = c(1, n_districts),
  ylim = c(0, 1),
  xlab = "District ID",
  ylab = "Predicted Proportion Using Contraception"
)

eps <- 1/3 # small value to separate lines

for (i in 1:n_districts) {
  lines(
    x = c(i, i),
    y = c(fixed_ci[i, 1], fixed_ci[i, 2]),
    col = col.alpha("darkorange", 0.7),
    lwd = 2
  )
  lines(
    x = c(i, i) + eps,
    y = c(varying_ci[i, 1], varying_ci[i, 2]),
    col = col.alpha("darkblue", 0.7),
    lwd = 2
  )
  points(
    x = i,
    y = fixed_mean[i],
    col = "darkorange",
    pch = 16
  )
  points(
    x = i + eps,
    y = varying_mean[i],
    col = "darkblue",
    pch = 16
  )
}

# add marginal mean
post_varying <- extract.samples(m_varying)
marginal_mean <- inv_logit(post_varying$a_bar)
CI_marginal <- PI(marginal_mean)

abline(
  h = CI_marginal[1],
  lty = 2,
  col = "black"
)
abline(
  h = CI_marginal[2],
  lty = 2,
  col = "black"
)

legend(
  "topright",
  legend = c("Fixed-effects model", "Varying-effects model", "Marginal mean"),
  col = c("darkorange", "darkblue", "black"),
  lty = c(1, 1, 2),
  lwd = 2
)
```

Across districts, the varying-effects predictions are closer to the marginal mean, i.e., less variable. We can see that the degree of shrinkage towards the mean is strongest for districts with less data.

```{r}
n_obs <- table(d$district_id)
shrinkage <- abs(fixed_mean - varying_mean)

plot(
  as.numeric(n_obs),
  shrinkage,
  pch = 16,
  xlab = "Number of Observations per District",
  ylab = "Absolute Shrinkage"
)
```

Also for districts whose raw proportions of women using contraception are close to the overall mean, the shrinkage is closer to 0.

```{r}
district_means <- aggregate(use_contraception ~ district_id, data = model_data, mean)

plot(
  district_means$use_contraception - mean(marginal_mean),
  shrinkage,
  pch = 16,
  xlab = "District level deviation from marginal mean",
  ylab = "Absolute shrinkage"
)
```


## Question 2:

Perform model of the fixed and varying effects models using PSIS-LOO. What do you conclude about their out-of-sample predictive accuracy? Are there any problematic districts flagged by the pareto-k values?

```{r}
PSIS_fixed <- PSIS(m_fixed, pointwise = TRUE)
PSIS_varying <- PSIS(m_varying, pointwise = TRUE)

compare(m_fixed, m_varying, func = PSIS)
```

```{r}
average_k_fixed <- aggregate(PSIS_fixed$k, by = list(district_id = model_data$district_id), max)

average_penalty_fixed <- aggregate(PSIS_fixed$penalty, by = list(district_id = model_data$district_id), mean)

average_k_varying <- aggregate(PSIS_varying$k, by = list(district_id = model_data$district_id), max)

average_penalty_varying <- aggregate(PSIS_varying$penalty, by = list(district_id = model_data$district_id), mean)

# plot pareto-k values against sample size

par(mfrow = c(2, 2))
plot(
  x = as.numeric(n_obs),
  y = average_k_fixed$x,
  pch = 16,
  xlab = "Number of Observations per District",
  ylab = "Maximum Pareto-k",
  main = "Fixed-effects model"
)

plot(
  x = as.numeric(n_obs),
  y = average_penalty_fixed$x,
  pch = 16,
  xlab = "Number of Observations per District",
  ylab = "Average Penalty",
  main = "Fixed-effects model"
)

plot(
  x = as.numeric(n_obs),
  y = average_k_varying$x,
  pch = 16,
  xlab = "Number of Observations per District",
  ylab = "Maximum Pareto-k",
  main = "Varying-effects model"
)

plot(
  x = as.numeric(n_obs),
  y = average_penalty_varying$x,
  pch = 16,
  xlab = "Number of Observations per District",
  ylab = "Average Penalty",
  main = "Varying-effects model"
)
```

## Question 3:

Fit a 3rd model that, instead of Gaussian-distributed varying effects, uses student-t distributed. This means including an additional degree of freedom parameter (`nu`) in the model, that you may either fix to a single value or estimate from the data. Then, compare both the predictions and PSIS-LOO values of the 3 models.

```{r}
m_varying_t <- ulam(
  alist(
    use_contraception ~ dbinom(1, p),
    logit(p) <- a[district_id],
    a_bar ~ dnorm(0, 1),
    a[district_id] ~ dstudent(nu, a_bar, sigma_a),
    sigma_a ~ dexp(1),
    nu ~ dnorm(3, 3)
  ),
  constraints=list(nu="lower=1"),
  data = model_data,
  chains = 4,
  cores = 4,
  log_lik = TRUE
)
```

```{r}
pred_varying_t <- link(m_varying_t, data = data.frame(district_id = sort(unique(model_data$district_id))))

varying_mean_t <- apply(pred_varying_t, 2, mean)
varying_ci_t <- t(apply(pred_varying_t, 2, PI))

dev.off()

plot(
  NULL,
  xlim = c(1, n_districts),
  ylim = c(0, 1),
  xlab = "District ID",
  ylab = "Predicted Proportion Using Contraception"
)

eps <- 1/3 # small value to separate lines

for (i in 1:n_districts) {
  lines(
    x = c(i, i),
    y = c(fixed_ci[i, 1], fixed_ci[i, 2]),
    col = col.alpha("darkorange", 0.7),
    lwd = 2
  )
  lines(
    x = c(i, i) + eps,
    y = c(varying_ci[i, 1], varying_ci[i, 2]),
    col = col.alpha("darkblue", 0.7),
    lwd = 2
  )
  lines(
    x = c(i, i) + eps,
    y = c(varying_ci_t[i, 1], varying_ci_t[i, 2]),
    col = col.alpha("darkgreen", 0.7),
    lwd = 2
  )
  points(
    x = i,
    y = fixed_mean[i],
    col = "darkorange",
    pch = 16
  )
  points(
    x = i + eps,
    y = varying_mean[i],
    col = "darkblue",
    pch = 16
  )
  points(
    x = i + eps,
    y = varying_mean_t[i],
    col = "darkgreen",
    pch = 16
  )
}

abline(
  h = CI_marginal[1],
  lty = 2,
  col = "black"
)
abline(
  h = CI_marginal[2],
  lty = 2,
  col = "black"
)

legend(
  "topright",
  legend = c("Fixed-effects model", "Varying-effects model", "Varying-effects model (t-distribution)", "Marginal mean"),
  col = c("darkorange", "darkblue", "darkgreen", "black"),
  lty = c(1, 1, 1, 2),
  lwd = 2
)
```

```{r}
compare(m_fixed, m_varying, m_varying_t, func = PSIS)
```

```{r}
PSIS_t <- PSIS(m_varying_t, pointwise = TRUE)

average_k_t <- aggregate(PSIS_t$k, by = list(district_id = model_data$district_id), max)

average_penalty_t <- aggregate(PSIS_t$penalty, by = list(district_id = model_data$district_id), mean)

plot(
  x = as.numeric(n_obs),
  y = average_k_t$x,
  pch = 16,
  xlab = "Number of Observations per District",
  ylab = "Maximum Pareto-k",
  main = "Varying-effects model (t-distribution)"
)
```


## Question 4:

Building upon your preferred model from the previous question, use the indicator variable of `urban` as a predictor of `use.contraception` in addition to district. How does the inclusion of this predictor change the magnitude of the district-level fixed or varying effects? Can you propose an explanation for why this might be?

```{r}
m_varying_urban <- ulam(
  alist(
    use_contraception ~ dbinom(1, p),
    logit(p) <- a_bar + a[district_id]*sigma_a + b_bar * urban,
    a_bar ~ dnorm(0, 0.5),
    b_bar ~ dnorm(0, 0.25),
    a[district_id] ~ dnorm(0, 1),
    sigma_a ~ dexp(1)
  ),
  data = model_data,
  chains = 4,
  cores = 4,
  cmdstan = TRUE
)
```

```{r}
precis(m_varying)
precis(m_varying_urban)

post_varying <- extract.samples(m_varying)
post_varying_urban <- extract.samples(m_varying_urban)

dens(post_varying$sigma_a, col = "blue", xlab = "sigma_a")
dens(post_varying_urban$sigma_a, add = TRUE, col = "red")
legend(
  "topright",
  legend = c("model without urban", "model with urban"),
  col = c("blue", "red"),
  lty = 1
)
```

The key here is that, because the intercept now refers to the non-urban districts, it is lower. sigma_a is slightly lower, but on the log-odds scale quite similar to the model without urban. Does this imply that urban does not capture much between-district variation? Not necessarily! Because the intercept is lower, a sigma of the same mangitude (on the log-odds scale) has less impact on the outcome scale.

