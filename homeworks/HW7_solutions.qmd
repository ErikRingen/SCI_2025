---
title: "SCI 2025: Homework 7"
format: 
  pdf:
    geometry: margin=1in
    fontsize: 12pt
    linestretch: 1.5
    include-in-header:
      text: |
        \usepackage{amsmath}
        \usepackage{amsfonts}
execute:
  echo: true
  warning: false
  message: false
---

## Question 1: 9H3

Sometimes changing a prior for one parameter has unanticipated effects on other parameters. This is because when a parameter is highly correlated with another parameter in the posterior, the prior influences both parameters. Here's an example to work and think through. Go back to the leg length example in Chapter 6 and use the code there to simulate height and leg lengths for 100 imagined individuals. Below is the model you fit before, resulting in a highly correlated posterior for the two beta parameters. This time, fit the model using `ulam`:

```{r, eval=FALSE}
library(rethinking)

N <- 100 # number of individuals
set.seed(909)
height <- rnorm(N,10,2) # sim total height of each
leg_prop <- runif(N,0.4,0.5) # leg as proportion of height
leg_left <- leg_prop*height + # sim left leg as proportion + error
    rnorm( N,0,0.02)
leg_right <- leg_prop*height + #sim right leg as proportion + error
    rnorm( N,0,0.02)
# combine into data frame
d <- data.frame(height,leg_left,leg_right)

m5.8s <- ulam(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + bl*leg_left + br*leg_right,
        a ~ dnorm(10,100),
        bl ~ dnorm(2,10),
        br ~ dnorm(2,10),
        sigma ~ dexp(1)
    ), data=d, chains=4,
    start=list(a=10, bl=0, br=0.1, sigma=1),
    log_lik=TRUE
    )
```

Let's take a look at the coordinates (posterior values) for `bl` and `br` over iterations from our chains.

```{r, eval=FALSE}
post <- extract.samples(m5.8s) # extract posterior samples

plot(post$bl[1:30], type = 'l', pch=16, col=rangi2, xlab="iteration",
ylab="parameter value", ylim = c(-8, 10))
points(post$bl[1:30], pch=16, col=rangi2)

lines(post$br[1:30], col="orange")
points(post$br[1:30], pch=16, col="orange")
legend("topright", legend=c("bl", "br"), col=c(rangi2, "orange"), lty=1, cex=0.8)
```

What do you notice about the positions of `br` and `bl` over iterations? Can you explain why this is happening?

## Question 2:

One way to improve the sampling of HMC with highly correlated parameters is to set a prior on the total prior contribution, plus a proportional allocation (e.g., 50% of the effect to right leg, 50% to left leg). See example below:

```{r, eval=FALSE}
m5.8s2 <- ulam(
    alist(
        height ~ dnorm(mu, sigma),
        transpars> bl <- b*theta,
        transpars> br <- b*(1 - theta),
        mu <- a + bl*leg_left + br*leg_right,
        a ~ dnorm(10,100),
        b ~ dnorm(2,10),
        theta ~ dbeta(2, 2),
        sigma ~ dexp(1)
    ), data=d, chains=4,
    start=list(a=10, b=1, theta=0.5, sigma=1),
    log_lik=TRUE
    )
```

`transpars` refers to the "transformed parameters" block in a Stan program, and can be used whenever we have parameters that are derived deterministically from other parameters. 

Take a look at the summary of the model above, as well as the first model we fit. Do you notice any differences in the HMC sampling given this new parameterization? What about warnings and sampling speed?

- Much faster sampling
- No max treedepth warnings

```{r}
summary(m5.8s)
summary(m5.8s2)
```

- Much lower posterior sd in the parameters br/bl.
- More that double the effective sample size in the second model for bl and br (but not other parameters).

## Question 3:

For the two models above, use WAIC or PSIS to compute the effective number of parameters (i.e., the penalty term). You will need to use `log_lik=TRUE` when fitting the model to instruct `ulam` to compute the terms that both WAIC and PSIS need.

```{r}
psis_m5.8s <- PSIS(m5.8s)
psis_m5.8s2 <- PSIS(m5.8s2)

psis_m5.8s$penalty
psis_m5.8s2$penalty

compare(m5.8s, m5.8s2)
```

Which model has lower effective number of parameters? Can you explain it? Which model has the better expected out-of-sample predictive accuracy?

- The second model has lower effective number of parameters.
- The second model has better expected out-of-sample predictive accuracy.
- Even though the second model has more parameters, it has lower predictive variance due to the sum to `b` constraint.

## Question 4

In question 2 I said that the variance decomposition approach I used (sometimes called a 'dirichlet decomposition' when generalied to more than two predictors) is a strategy for handling highly correlated predictors. What assumptions are we making when we use this approach? What potential downsides or limitations occur to you?

- We are assuming that both parameters contribute in the same direction to the outcome (i.e., both are positive).
- It is often less intuitive how to set a prior on the total contribution of the parameters (but can we worked out arithmetically).
- The above point is magnified whenever we have a link function, introducing non-linearity.

## Question 5

Re-fit the model in question 2, but this time make the prior for `theta` concentrated around most of the effect going to the right leg (if you're not sure how, try simulating some values from a beta distribution with `rbeta()`). Do you notice and differences in sampling, and effective number of parameters? Can you explain it?

```{r}
m5.8s3 <- ulam(
    alist(
        height ~ dnorm(mu, sigma),
        transpars> bl <- b*theta,
        transpars> br <- b*(1 - theta),
        mu <- a + bl*leg_left + br*leg_right,
        a ~ dnorm(10,100),
        b ~ dnorm(2,10),
        theta ~ dbeta(1, 5),
        sigma ~ dexp(1)
    ), data=d, chains=4,
    log_lik=TRUE
)

summary(m5.8s3)
```

```{r}
compare(m5.8s2, m5.8s3)
```

- Most of the effect of legs is shifted to the right leg, even though the variables convey (effectively) the same information.
- Both models have the same effective number of parameters.
- The models are nearly tied for out-of-sample predictive accuracy.
- Both models make the same predictions using approximately the same complexity, it it just a matter of where you want to allocate the effect (i.e., distributed over legs vs concentrated in one leg).
